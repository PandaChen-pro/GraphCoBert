# GraphCodeBERT 克隆检测项目错误和解决方案

## 错误1：依赖包安装失败

### 问题描述
```bash
pip install torch transformers tree_sitter sklearn
```
执行时出现错误：
```
The 'sklearn' PyPI package is deprecated, use 'scikit-learn'
rather than 'sklearn' for pip commands.
```

### 解决方案
将 `sklearn` 替换为 `scikit-learn`：
```bash
pip install torch transformers tree_sitter scikit-learn
```

### 原因分析
`sklearn` 包已被弃用，需要使用 `scikit-learn` 替代。

---

## 错误2：Tokenizer加载失败

### 问题描述
运行训练脚本时出现错误：
```
OSError: Incorrect path_or_model_id: ''. Please provide either the path to a local folder or the repo_id of a model on the Hub.
```

### 解决方案
修改 `run.py` 第597行：
```python
# 修复前
tokenizer = RobertaTokenizer.from_pretrained(args.tokenizer_name)

# 修复后
tokenizer = RobertaTokenizer.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path)
```

### 原因分析
`tokenizer_name` 参数默认为空字符串，但代码直接使用该参数而没有检查是否为空。修复后与 config 加载逻辑保持一致。

---

## 错误3：训练损失变成NaN

### 问题描述
训练过程中出现：
```
epoch 0 loss nan: 3%|██▏ | 71/2500 [01:20<45:42, 1.13s/it
```

### 解决方案

#### 3.1 修复数值不稳定的损失计算
修改 `run.py` 第401行：
```python
# 修复前
avg_loss=round(np.exp((tr_loss - logging_loss) /(global_step- tr_nb)),4)

# 修复后
avg_loss=round((tr_loss - logging_loss) /(global_step- tr_nb),4)
```

#### 3.2 改进模型数值稳定性
修改 `model.py` 第49行：
```python
# 修复前
nodes_to_token_mask=nodes_to_token_mask/(nodes_to_token_mask.sum(-1)+1e-10)[:,:,None]

# 修复后
nodes_to_token_mask=nodes_to_token_mask/(nodes_to_token_mask.sum(-1)+1e-8)[:,:,None]
```

#### 3.3 添加损失值检查
修改 `run.py` 第386-388行：
```python
# 修复前
tr_loss += loss.item()
tr_num+=1
train_loss+=loss.item()

# 修复后
loss_value = loss.item()
if not (torch.isnan(loss) or torch.isinf(loss)):
    tr_loss += loss_value
    tr_num+=1
    train_loss+=loss_value
else:
    logger.warning("NaN or Inf loss detected, skipping this batch")
```

### 原因分析
1. **数值溢出**：`np.exp()` 函数在输入值过大时会返回无穷大或NaN
2. **除零问题**：防除零的小数值过小可能导致数值不稳定
3. **梯度爆炸**：需要适当的梯度裁剪和学习率设置

---

## 推荐的训练参数

为避免数值不稳定问题，建议使用以下参数：

```bash
python run.py \
    --output_dir=output/ \
    --model_name_or_path=microsoft/graphcodebert-base \
    --train_data_file=dataset/train.small.txt \
    --eval_data_file=dataset/valid.txt \
    --test_data_file=dataset/test.txt \
    --do_train \
    --do_eval \
    --epoch 1 \
    --code_length 384 \
    --data_flow_length 64 \
    --train_batch_size 4 \
    --eval_batch_size 8 \
    --learning_rate 1e-5 \
    --max_grad_norm 0.5 \
    --warmup_steps 100
```

### 参数调整说明
- **学习率**：从 `2e-5` 降低到 `1e-5`，减少数值不稳定风险
- **批次大小**：从 8 降低到 4，减少内存压力
- **梯度裁剪**：从 1.0 降低到 0.5，更严格的梯度控制
- **预热步数**：添加 100 步预热，平滑学习率变化

---

## 预防措施

1. **依赖管理**：使用正确的包名，避免使用已弃用的包
2. **参数检查**：确保所有必需参数都有合理的默认值或检查机制
3. **数值稳定性**：
   - 避免使用可能导致溢出的数学函数
   - 使用适当的防除零小数值
   - 添加NaN/Inf检查机制
4. **训练监控**：定期检查损失值，及时发现异常

---

## 调试技巧

1. **逐步调试**：从小数据集开始测试
2. **参数调优**：逐步调整学习率和批次大小
3. **日志监控**：密切关注训练日志中的异常值
4. **检查点保存**：定期保存模型检查点，避免长时间训练后的损失

---

## 错误4：除零错误 (ZeroDivisionError)

### 问题描述
修复NaN检查后出现新错误：
```
ZeroDivisionError: division by zero
```
在第396行：`avg_loss=round(train_loss/tr_num,5)`

### 解决方案

#### 4.1 修复除零错误
修改 `run.py` 第396行和第405行：
```python
# 修复前
avg_loss=round(train_loss/tr_num,5)
avg_loss=round((tr_loss - logging_loss) /(global_step- tr_nb),4)

# 修复后
avg_loss=round(train_loss/tr_num,5) if tr_num > 0 else 0.0
avg_loss=round((tr_loss - logging_loss) /(global_step- tr_nb),4) if (global_step- tr_nb) > 0 else 0.0
```

#### 4.2 修复模型配置错误
修改 `run.py` 第600行：
```python
# 修复前
config.num_labels=1

# 修复后
config.num_labels=2
```

#### 4.3 修复注意力掩码问题（关键修复）
修改 `model.py` 第53行：
```python
# 修复前
outputs = self.encoder.roberta(inputs_embeds=inputs_embeddings,attention_mask=attn_mask,position_ids=position_idx,token_type_ids=position_idx.eq(-1).long())[0]

# 修复后
# Convert 3D attention mask to 2D for RoBERTa
attention_mask_2d = attn_mask.any(dim=-1).float()
outputs = self.encoder.roberta(inputs_embeds=inputs_embeddings,attention_mask=attention_mask_2d,position_ids=position_idx,token_type_ids=position_idx.eq(-1).long())[0]
```

### 原因分析
1. **除零错误**：当所有批次的损失都是NaN时，`tr_num`保持为0，导致除零错误
2. **模型配置错误**：二分类任务应该使用`num_labels=2`而不是1
3. **注意力掩码维度错误**：RoBERTa期望2D注意力掩码，但代码传入了3D掩码，导致数值不稳定
4. **数据流问题**：NaN损失主要由注意力掩码维度错误引起

---

## 错误5：评估阶段卡住无进度

### 问题描述
训练过程中评估阶段看起来卡住了，没有进度显示：
```
06/23/2025 06:35:53 - INFO - __main__ -   ***** Running evaluation *****
06/23/2025 06:35:53 - INFO - __main__ -     Num examples = 41541
06/23/2025 06:35:53 - INFO - __main__ -     Batch size = 8
epoch 0 loss 0.65842:  10%|█████                                              | 499/5000 [17:24<2:37:03,  2.09s/it
```

### 解决方案

#### 5.1 添加评估进度条
修改 `run.py` 第449行和第506行：
```python
# 修复前
for batch in eval_dataloader:

# 修复后
for batch in tqdm(eval_dataloader, desc="Evaluating"):
```

#### 5.2 调整评估频率
修改 `run.py` 第332行：

**测试阶段（当前设置）**：
```python
args.save_steps=500  # 每500步评估一次，用于测试各模块是否正常
```

**生产阶段（推荐设置）**：
```python
args.save_steps=len(train_dataloader)  # 每个epoch评估一次，用于正式训练
```

**原始设置（过于频繁）**：
```python
args.save_steps=len(train_dataloader)//10  # 每10%训练数据评估一次，太频繁
```

#### 5.3 训练时使用评估数据子集
修改 `run.py` evaluate函数：
```python
# 在训练期间使用更小的评估数据集
if eval_when_training:
    subset_size = min(len(eval_dataset) // 5, 5000)  # 最多5000个样本
    indices = list(range(0, len(eval_dataset), len(eval_dataset) // subset_size))[:subset_size]
    eval_dataset = torch.utils.data.Subset(eval_dataset, indices)
```

#### 5.4 添加内存清理
```python
# 定期清理GPU缓存
if nb_eval_steps % 100 == 0:
    torch.cuda.empty_cache()
```

### 原因分析
1. **评估数据集过大**：41541个样本的评估需要很长时间
2. **评估频率过高**：每10%训练数据就评估一次，对大数据集来说太频繁
3. **缺少进度显示**：用户无法知道评估是否在进行
4. **内存累积**：长时间评估可能导致GPU内存问题

---

## 错误6：DataLoader多进程错误

### 问题描述
训练或评估时出现多进程相关错误：
```
RuntimeError: DataLoader worker (pid 138252) is killed by signal: Bus error.
It is possible that dataloader's workers are out of shared memory.
Please try to raise your shared memory limit.
```

### 解决方案

#### 6.1 禁用多进程数据加载
修改所有DataLoader的 `num_workers` 参数：

**训练数据加载器** (第349-350行)：
```python
# 修复前
train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size,num_workers=4)

# 修复后
train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size,num_workers=0)
```

**评估数据加载器** (第489-490行)：
```python
# 修复前
eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler,batch_size=args.eval_batch_size,num_workers=4)

# 修复后
eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler,batch_size=args.eval_batch_size,num_workers=0)
```

**测试数据加载器** (第562行)：
```python
# 修复前
eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=4)

# 修复后
eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=0)
```

### 原因分析
1. **共享内存不足**：多进程数据加载需要大量共享内存
2. **进程间通信问题**：Docker容器或某些环境下多进程通信可能不稳定
3. **资源竞争**：多个worker进程可能导致资源竞争

### 性能影响
- **优点**：避免多进程相关错误，更稳定
- **缺点**：数据加载速度可能稍慢（通常影响不大）
- **建议**：对于中小型数据集，单进程加载通常足够

---

*最后更新：2025-06-24*